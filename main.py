# -*- coding: utf-8 -*-
"""Bike_Sharing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1huleUav4CiRsQqje3onFzVnnQS67UuFp

# Multiple Linear Regression
## Shared Bikes Demand Prediction

#### Problem Statement:

A US bike-sharing provider `BoomBikes` has a daily dataset on the rental bikes based on various environmental and seasonal settings. It wishes to use this data to understand the factors affecting the demand for these shared bikes in the American market and come up with a mindful business plan to be able to accelerate its revenue as soon as the ongoing lockdown due to corona pandemic comes to an end.

Essentially, the company wants to know â€”


- Which variables are significant in predicting the demand for shared bikes.


- How well those variables describe the bike demands


**So interpretation is important!**

The solution is divided into the following sections:
- Data understanding and exploration
- Data Visualisation
- Data preparation
- Model building and evaluation

### 1. Data Understanding and Exploration

Let's first import the required libraries and have a look at the dataset and understand the size, attribute names etc.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import linear_model
from sklearn.linear_model import LinearRegression

import warnings
warnings.filterwarnings('ignore')

from google.colab import files
uploaded = files.upload()

import pandas as pd

# Assuming the uploaded file is named "day.csv"
df = pd.read_csv("day.csv")

print(df)

# Let's look at the number of rows and columns in the dataset
df.shape

# Understanding the feature names in the dataset
df.columns

# Getting insights of the features
df.describe()

# Summary of the dataset: 730 rows, 16 columns, no null values
print(df.info())

"""#### Understanding the Data Dictionary and parts of Data Preparation

The data dictionary contains the meaning of various attributes; some of which are explored and manipulated here:
"""

# Assigning string values to different seasons instead of numeric values. These numeric values may misindicate some order to it.

# 1=spring
df.loc[(df['season'] == 1) , 'season'] = 'spring'

# 2=summer
df.loc[(df['season'] == 2) , 'season'] = 'summer'

# 3=fall
df.loc[(df['season'] == 3) , 'season'] = 'fall'

# 4=winter
df.loc[(df['season'] == 4) , 'season'] = 'winter'

# Checking whether the conversion is done properly or not and getting data count on the basis of season
df['season'].astype('category').value_counts()

# year (0: 2018, 1:2019)
df['yr'].astype('category').value_counts()

# Assigning string values to different months instead of numeric values which may misindicate some order to it.
# A function has been created to map the actual numbers to categorical levels.
def object_map(x):
    return x.map({1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun', 7: 'Jul',8: 'Aug',9: 'Sept',10: 'Oct',11: 'Nov',12: 'Dec'})

# Applying the function to the two columns
df[['mnth']] = df[['mnth']].apply(object_map)

# Checking whether the conversion is done properly or not and getting data count on the basis of month
df['mnth'].astype('category').value_counts()

# whether day is a holiday or not (0: No, 1: Yes)
df['holiday'].astype('category').value_counts()

# Assigning string values to weekdays instead of numeric values. These values may misindicate some order to it.
# A function has been created to map the actual numbers to categorical levels.
def str_map(x):
    return x.map({1: 'Wed', 2: 'Thurs', 3: 'Fri', 4: 'Sat', 5: 'Sun', 6: 'Mon', 0: 'Tues'})

# Applying the function to the two columns
df[['weekday']] = df[['weekday']].apply(str_map)

# Checking whether the conversion is done properly or not and getting data count on the basis of weekdays
df['weekday'].astype('category').value_counts()

# if a day is neither weekend nor a holiday it takes the value 1, otherwise 0
df['workingday'].astype('category').value_counts()

# Replacing long weathersit names into string values for better readability and understanding

# 1-Clear, Few clouds, Partly cloudy, Partly cloudy
df.loc[(df['weathersit'] == 1) , 'weathersit'] = 'A'

# 2-Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist
df.loc[(df['weathersit'] == 2) , 'weathersit'] = 'B'

# 3-Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds
df.loc[(df['weathersit'] == 3) , 'weathersit'] = 'C'

# 4-Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog
df.loc[(df['weathersit'] == 4) , 'weathersit'] = 'D'

# Extracting the type of weather situations present in the data
df['weathersit'].unique()

# Taking count based on weather situations
df['weathersit'].astype('category').value_counts()

"""### 2. Data Visualisation

Let's now spend some time doing what is arguably the most important step - **understanding the data**.
- Understanding the distribution of various numeric variables
- If there is some obvious multicollinearity going on, this is the first place to catch it
- Here's where you'll also identify if some predictors directly have a strong association with the outcome variable

We'll visualise our data using `matplotlib` and `seaborn`.
"""

# temperature
sns.distplot(df['temp'])
plt.show()

# feeling temperature
sns.distplot(df['atemp'])
plt.show()

# humidity
sns.distplot(df['hum'])
plt.show()

# wind speed
sns.distplot(df['windspeed'])
plt.show()

# Target variable: count of total rental bikes including both casual and registered
sns.distplot(df['cnt'])
plt.show()

df['dteday'] = pd.to_datetime(df['dteday'], errors='coerce')

# All categorical variables in the dataset
BS_day_categorical=df.select_dtypes(exclude=['float64','datetime64','int64'])
print(BS_day_categorical.columns)

BS_day_categorical

"""#### Visualising Categorical Variables

As you might have noticed, there are a few categorical variables as well. Let's make a boxplot for some of these variables.
"""

plt.figure(figsize=(20, 20))
plt.subplot(3,3,1)
sns.boxplot(x = 'season', y = 'cnt', data = df)
plt.subplot(3,3,2)
sns.boxplot(x = 'holiday', y = 'cnt', data = df)
plt.subplot(3,3,3)
sns.boxplot(x = 'workingday', y = 'cnt', data = df)
plt.subplot(3,3,4)
sns.boxplot(x = 'weathersit', y = 'cnt', data = df)
plt.subplot(3,3,5)
sns.boxplot(x = 'mnth', y = 'cnt', data = df)
plt.subplot(3,3,6)
sns.boxplot(x = 'weekday', y = 'cnt', data = df)
plt.subplot(3,3,7)
sns.boxplot(x = 'yr', y = 'cnt', data = df)
plt.show()
#plt.subplot(3,3,8)
#sns.boxplot(x = 'date', y = 'cnt', data = BS_day)

"""#### Visualising Numeric Variables

Let's make a pairplot of all the numeric variables
"""

# Converting "casual","registered" and "cnt" numeric variables to float.
# This step is performed to seperate out categorical variables like 'yr','holiday','workingday' which have binary values in them
IntVariableList = ["casual","registered","cnt"]

for var in IntVariableList:
    df[var] = df[var].astype("float")
#BS_day['casual']= BS_day['casual'].astype('float')
#BS_day['registered']=BS_day['registered'].astype('float')
#BS_day['cnt']=BS_day['cnt'].astype('float')
#BS_day.head()

# All numeric variables in the dataset
BS_day_numeric = df.select_dtypes(include=['float64'])
BS_day_numeric.head()

# Pairwise scatter plot
sns.pairplot(BS_day_numeric)
plt.show()

"""We can better plot correlation matrix between variables to know the exact values of correlation between them. Also, a heatmap is pretty useful to visualise multiple correlations in one plot."""

# Correlation matrix
cor = BS_day_numeric.corr()
cor

"""Let's plot the correlations on a heatmap for better visualisation"""

# heatmap
mask = np.array(cor)
mask[np.tril_indices_from(mask)] = False
fig,ax= plt.subplots()
fig.set_size_inches(10,10)
sns.heatmap(cor, mask=mask,vmax=.8, square=True,annot=True)

"""The heatmap shows some useful insights:

Correlation of Count('cnt') with independent variables:
- Count('cnt') is highly (positively) correlated with 'casual' and 'registered' and further it is high with 'atemp'. We can clearly understand the high positive correlation of count with 'registered' and 'casual' as both of them together add up to represent count.

- Count is negatively correlated to 'windspeed' (-0.24 approximately). This gives us an impression that the shared bikes demand will be somewhat less on windy days as compared to normal days.

Correlation among independent variables:
- Some of the independent variables are highly correlated (look at the top-left part of matrix): atemp and temp are highly (positively) correlated. The correlation between the two is almost equal to 1.


Thus, while building the model, we'll have to pay attention to multicollinearity.
"""

#removing atemp as it is highly correlated with temp
BS_day_numeric.drop('atemp',axis=1,inplace=True)

"""## 3. Data Preparation


#### Data Preparation

Let's now prepare the data and build the model.
Note that we had not included 'yr', 'mnth', 'holiday', 'weekday' and 'workingday' as object variables in the initial data exploration steps so as to avoid too many dummy variables creation. They have binary values: 0s and 1s in them which have specific meanings associated with them.
"""

# Print the DataFrame BS_day_numeric to inspect its contents
print(BS_day_numeric.head())

# Check the data types of columns in BS_day_numeric
print(BS_day_numeric.dtypes)

# Subset all categorical variables
BS_day_categorical = BS_day_numeric.select_dtypes(include=['object'])

# Print BS_day_categorical to inspect its contents
print(BS_day_categorical.head())

"""#### Dummy Variables
The variable `season`,`mnth`,`weekday` and `weathersit` have different levels. We need to convert these levels into integers.

For this, we will use something called `dummy variables`.
"""

# Print the data types of all columns in BS_day_numeric
print(BS_day_numeric.dtypes)

# Inspect the contents of BS_day_numeric to understand its structure
print(BS_day_numeric.head())
# Check if BS_day_numeric contains any columns with object data type (categorical variables)
if 'object' in BS_day_numeric.dtypes.values:
    # Subset all categorical variables
    BS_day_categorical = BS_day_numeric.select_dtypes(include=['object'])

    # Check if BS_day_categorical is not empty
    if not BS_day_categorical.empty:
        # Convert categorical variables into dummy variables
        BS_day_dummies = pd.get_dummies(BS_day_categorical, drop_first=True)
        print(BS_day_dummies.head())  # Print the head of BS_day_dummies for inspection
    else:
        print("No categorical variables found in BS_day_numeric. Please check the data.")
else:
    print("No columns with object data type (categorical variables) found in BS_day_numeric.")

# Drop categorical variable columns
BS_day_numeric = BS_day_numeric.drop(list(BS_day_categorical.columns), axis=1)

# Assuming BS_day_categorical is defined and contains categorical variables
if not BS_day_categorical.empty:
    # Convert categorical variables into dummy variables
    BS_day_dummies = pd.get_dummies(BS_day_categorical, drop_first=True)
    print(BS_day_dummies.head())  # Print the head of BS_day_dummies for inspection

    # Assuming BS_day is defined and contains valid data
    # Concatenate dummy variables with the original DataFrame
    BS_day = pd.concat([BS_day, BS_day_dummies], axis=1)
    print("Concatenation successful.")
else:
    print("BS_day_categorical is empty. Please check the data.")

# Let's check the first few rows
BS_day.head()

# Drop the 'instant' and 'dteday' column as they of not any use to us for the analysis
BS_day=BS_day.drop(['instant','dteday'], axis = 1, inplace = False)
BS_day.head()

"""## 4. Model Building and Evaluation

Let's start building the model. The first step to model building is the usual test-train split. So let's perform that
"""

# Split the dataframe into train and test sets
from sklearn.model_selection import train_test_split
np.random.seed(0)
df_train, df_test = train_test_split(BS_day, train_size=0.7, test_size=0.3, random_state=100)

df_train

"""### Scaling

Now that we have done the test-train split, we need to scale the variables for better interpretability. But we only need the scale the numeric columns and not the dummy variables. Let's take a look at the list of numeric variables we had created in the beginning. Also, the scaling has to be done only on the train dataset as you don't want it to learn anything from the test data.

Let's scale all these columns using MinMaxScaler. You can use any other scaling method as well; it is totally up to you.
"""

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

# Apply scaler() to all the columns except the 'yes-no' and 'dummy' variables
var = ['temp', 'hum', 'windspeed','casual','registered','cnt']

df_train[var] = scaler.fit_transform(df_train[var])

df_train

"""As expected, the variables have been appropriately scaled."""

df_train.describe()

# Let's check the correlation coefficients to see which variables are highly correlated
plt.figure(figsize = (30, 30))
sns.heatmap(df_train.corr(), annot = True, cmap="YlGnBu")
plt.show()

"""As you might have noticed, `temp` seems to the correlated to `cnt` the most, after 'casual' and 'registered'. Let's see a pairplot for `temp` vs `cnt`."""

plt.figure(figsize=[6,6])
plt.scatter(df_train.temp, df_train.cnt)
plt.show()

"""#### Dividing into X and Y sets for the model building"""

# Dropping 'casual' and 'registered' as together they add up to cnt
y_train = df_train.pop('cnt')
X_train = df_train.drop(["casual","registered"],axis=1)

X_train.head()

# This is done to convert all the features into array before fitting the model and avoid any error popping up
np.asarray(df_train)

X_train.shape

"""### Building the first model with all the features

Let's now build our first model with all the features.
"""

import statsmodels.api as sm
X_train_lm = sm.add_constant(X_train)

lr = sm.OLS(y_train, X_train_lm).fit()

lr.params

# Instantiate
lm = LinearRegression()

# Fit a line
lm.fit(X_train, y_train)

# Print the coefficients and intercept
print(lm.coef_)
print(lm.intercept_)

# getting the model summary
lr.summary()

"""This model has an Adjusted R-squared value of **84.5%** which seems pretty good. But let's see if we can reduce the number of features and exclude those which are not much relevant in explaining the target variable.

#### Model Building Using RFE

Now, you have close to 28 features. It is obviously not recommended to manually eliminate these features. So let's now build a model using recursive feature elimination to select features. We'll first start off with an arbitrary number of features (15 seems to be a good number to begin with), and then use the `statsmodels` library to build models using the shortlisted features (this is also because `SKLearn` doesn't have `Adjusted R-squared` that `statsmodels` has).
"""

# Import RFE - Recursive Feature Elimination
from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression

# Assuming X_train and y_train are defined and contain the training features and target respectively

# RFE with 15 features
lm = LinearRegression()
rfe1 = RFE(estimator=lm, n_features_to_select=15)  # Provide estimator and number of features to select

# Fit with 15 features
rfe1.fit(X_train, y_train)

# Print the boolean results
print(rfe1.support_)
print(rfe1.ranking_)

"""#### Model Building and Evaluation

Let's now check the summary of this model using `statsmodels`.
"""

# Import statsmodels
import statsmodels.api as sm

# Subset the features selected by rfe1
col1 = X_train.columns[rfe1.support_]

# Subsetting training data for 15 selected columns
X_train_rfe1 = X_train[col1]

# Add a constant to the model
X_train_rfe1 = sm.add_constant(X_train_rfe1)
X_train_rfe1.head()

# Fitting the model with 15 variables
lm1 = sm.OLS(y_train, X_train_rfe1).fit()
print(lm1.summary())

"""Note that the new model built on the selected features doesn't show much dip in the accuracy in comparison to the model which was built on all the features. It has gone from **84.5%** to **84.4%**. This is indeed a good indication to proceed with these selected features.

But let's check for the multicollinearity among these variables.
"""

# Check for the VIF values of the feature variables.
from statsmodels.stats.outliers_influence import variance_inflation_factor

"""# Variance Inflation Factor

Variance Inflation Factor (VIF) measures the multicollinearity among predictor variables in a regression model. Multicollinearity occurs when predictor variables in a regression model are highly correlated with each other. This can cause issues such as inflated standard errors of coefficients, which can lead to inaccurate hypothesis testing and unreliable interpretations of the model.

By calculating VIF values for each feature variable, you can assess the extent of multicollinearity in your regression model. Generally, a VIF value greater than 5 or 10 indicates a high degree of multicollinearity, suggesting that the corresponding variable may need to be addressed, either by removing it from the model or by using techniques like principal component analysis (PCA) to address multicollinearity.
"""

a=X_train_rfe1.drop('const',axis=1)

# Create a dataframe that will contain the names of all the feature variables and their respective VIFs except for the constant
vif = pd.DataFrame()
vif['Features'] = a.columns
vif['VIF'] = [variance_inflation_factor(a.values, i) for i in range(a.shape[1])]
vif['VIF'] = round(vif['VIF'], 2)
vif = vif.sort_values(by = "VIF", ascending = False)
vif

# Import RFE
from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression

# Assuming X_train and y_train are defined and contain the training features and target respectively

# RFE with 7 features
lm = LinearRegression()
rfe2 = RFE(estimator=lm, n_features_to_select=7)  # Provide estimator and number of features to select

# Fit with 7 features
rfe2.fit(X_train, y_train)

# Print the boolean results
print(rfe2.support_)
print(rfe2.ranking_)

# Import statsmodels
import statsmodels.api as sm

# Subset the features selected by rfe1
col1 = X_train.columns[rfe2.support_]

# Subsetting training data for 7 selected columns
X_train_rfe2 = X_train[col1]

# Add a constant to the model
X_train_rfe2 = sm.add_constant(X_train_rfe2)
X_train_rfe2.head()

# Fitting the model with 7 variables
lm2 = sm.OLS(y_train, X_train_rfe2).fit()
print(lm2.summary())

"""Now let's check the VIF for these selected features and decide further."""

b=X_train_rfe2.drop('const',axis=1)

# Create a dataframe that will contain the names of all the feature variables and their respective VIFs except for the constant
vif = pd.DataFrame()
vif['Features'] = b.columns
vif['VIF'] = [variance_inflation_factor(b.values, i) for i in range(b.shape[1])]
vif['VIF'] = round(vif['VIF'], 2)
vif = vif.sort_values(by = "VIF", ascending = False)
vif

"""From the model summary above, all the variables have p-value < 0.05 and from the p-value perspective, all variables seem significant. But notice that there are a few variables which have VIF > 5. We need to deal with these variables carefully.

So let's try removing 'hum' first having the maximum VIF and then check for it again. Dropping this variable may result in a change in other VIFs which are high.
"""

# Let's drop the 'hum' column
X_train_rfe2.drop("hum",axis=1,inplace=True)
X_train_rfe2

X_train_rfe2 = sm.add_constant(X_train_rfe2)

# Now that we have removed one variable, let's fit the model with 6 variables
lm3 = sm.OLS(y_train, X_train_rfe2).fit()
print(lm3.summary())

"""The model seems to be doing a good job. Let's also quickly take a look at the VIF values."""

c=X_train_rfe2.drop('const',axis=1)

# Create a dataframe that will contain the names of all the feature variables and their respective VIFs except for the constant
vif = pd.DataFrame()
vif['Features'] = c.columns
vif['VIF'] = [variance_inflation_factor(c.values, i) for i in range(c.shape[1])]
vif['VIF'] = round(vif['VIF'], 2)
vif = vif.sort_values(by = "VIF", ascending = False)
vif

"""All the VIF values and p-values seem to be in the permissible range now. Also the `Adjusted R-squared` value has dropped from `84.5%` with **28 variables** to just `79.1%` using **6 variables**. This model is explaining most of the variance without being too complex. So let's proceed with this model.

### Residual Analysis

Before we make predictions on the test set, let's first analyse the residuals.
"""

y_train_cnt = lm3.predict(X_train_rfe2)

# Plot the histogram of the error terms
fig = plt.figure()
sns.distplot((y_train - y_train_cnt), bins = 20)
# Plot heading
fig.suptitle('Error Terms', fontsize = 20)
# Give the X-label
plt.xlabel('Errors', fontsize = 18)

"""The error terms are fairly normally distributed and we can surely live with this. Let's now make predictions on the test-set.

### Making Predictions

We would first need to scale the test set as well. So let's start with that.
"""

X_train_rfe2

# let's recall the set of variables which are to be scaled
var

df_test[var] = scaler.transform(df_test[var])

# Split the 'df_test' set into X and y after scaling
y_test = df_test.pop('cnt')
X_test = df_test.drop(["casual","registered"],axis=1)

X_test.head()

# Let's check the list 'col2' which had the 6 variables RFE had selected
col2=c.columns
col2

# Let's subset these columns and create a new dataframe 'X_test_rfe1'
X_test_rfe2 = X_test[col2]

# Add a constant to the test set created
X_test_rfe2 = sm.add_constant(X_test_rfe2)
X_test_rfe2.info()

# Making predictions
y_pred = lm3.predict(X_test_rfe2)

# Plotting y_test and y_pred to understand the spread

fig = plt.figure()
plt.scatter(y_test, y_pred)
fig.suptitle('y_test vs y_pred', fontsize = 20)              # Plot heading
plt.xlabel('y_test', fontsize = 18)                          # X-label
plt.ylabel('y_pred', fontsize = 16)

"""From the above plot, it's evident that the model is doing well on the test set as well. Let's also check the R-squared and more importantly, the adjusted R-squared value for the test set."""

# r2_score for 6 variables
from sklearn.metrics import r2_score
r2_score(y_test, y_pred)

"""Thus, for the model with 6 variables, the r-squared on training and test data is about 79.3% and 78.02% respectively. The adjusted r-squared on the train set is about is about 79.1%.

#### Checking the correlations between the final predictor variables
"""

# Figure size
plt.figure(figsize=(8,5))

# Heatmap
sns.heatmap(BS_day[col2].corr(), cmap="YlGnBu", annot=True)
plt.show()

"""This is the simplest model that we could build. The final predictors seem to have fairly low correlations.

Thus, the final model consists of the 6 variables mentioned above.One can go ahead with this model and use it for predicting count of daily bike rentals.


"""
